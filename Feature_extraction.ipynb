{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10Czu9DjaPk0x77fJezQcvBn5Je0yRnKF",
      "authorship_tag": "ABX9TyNn0v7Lhc1dc05oVn1fZPHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikhharsiingh/SwiftChat/blob/main/Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JmYPRK3Vabx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707eac57-f4f6-4fff-9e7f-a42a0bb0533f"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0mekvAjgeK0"
      },
      "source": [
        "#Reading Datasets\n",
        "def read_dataset(file):\n",
        "  dataset = pd.read_csv(file)\n",
        "  texts = dataset.iloc[:, 0]\n",
        "\n",
        "  return texts\n",
        "\n",
        "def converttolower(texts):\n",
        "  corpus = []\n",
        "  for line in texts:\n",
        "    line = line.lower()\n",
        "    corpus.append(line)\n",
        "  return corpus\n",
        "\n",
        "def rempunct(texts):\n",
        "  corpus = []\n",
        "  for line in texts:\n",
        "    opt = re.sub(r'[^\\w\\s]', '', line)\n",
        "    corpus.append(opt)\n",
        "    \n",
        "  return corpus\n",
        "\n",
        "def filter_nouns(corpus):\n",
        "  filtered = []\n",
        "  for line in corpus:\n",
        "    opt = nltk.tag.pos_tag(line.split())\n",
        "    eopt = [word for word,tag in opt if tag != 'NNP' and tag != 'NNPS']\n",
        "    filtered.append(' '.join(eopt))\n",
        "\n",
        "  return filtered\n",
        "\n",
        "def tokenize_csv(corpus):\n",
        "  ret = []\n",
        "  for text in corpus:\n",
        "    temp = nltk.word_tokenize(text)\n",
        "    ret.append(temp)\n",
        "\n",
        "  return ret\n",
        "\n",
        "def tokenize_txt(text):\n",
        "  if text is not None: \n",
        "    temp = nltk.word_tokenize(text)\n",
        "\n",
        "  return temp\n",
        "\n",
        "def lemmatize(corpus):\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  linesn = []\n",
        "  for line in corpus:\n",
        "    lemmas = []\n",
        "    for word in line:\n",
        "      temp = lemmatizer.lemmatize(word)\n",
        "      lemmas.append(temp)\n",
        "    linesn.append(lemmas)\n",
        "\n",
        "  return linesn\n",
        "\n",
        "def create_nphrase(corpus, n):\n",
        "  datapoint = []\n",
        "  for line in corpus:\n",
        "    for i in range(len(line) - n + 1):\n",
        "      temp = ''\n",
        "      for j in range (i, i + n):\n",
        "        temp = temp + line[j] + ' '\n",
        "      temp = ' ' + temp\n",
        "      datapoint.append(temp)\n",
        "\n",
        "  return datapoint\n",
        "\n",
        "def remove_stopwords(corpus):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered = []\n",
        "  for line in corpus:\n",
        "    for word in line:\n",
        "      if not word in stop_words:\n",
        "        filtered.append(word)\n",
        "\n",
        "  return filtered\n",
        "\n",
        "def make_features(corpus, threshold):\n",
        "  freq = FreqDist()\n",
        "\n",
        "  for token in corpus:\n",
        "    freq[token.lower()] += 1\n",
        "  top = freq.most_common(1000000)\n",
        "  #top10 = freq.most_common(10)\n",
        "  #print(top10)\n",
        "\n",
        "  features = []\n",
        "  temp = None\n",
        "  for item in top:\n",
        "    if item[1] >= threshold:\n",
        "      features.append(item[0])\n",
        "      temp = item\n",
        "  print(temp)\n",
        "  print(len(features))\n",
        "\n",
        "  return features\n",
        "\n",
        "def addspaces(tokens):\n",
        "  words = []\n",
        "  for token in tokens:\n",
        "    temp = \" \" + token + \" \"\n",
        "    words.append(temp)\n",
        "  return words\n",
        "\n",
        "def features_csv(file, thresh1, thresh2, thresh3, lemma = 0):\n",
        "  #Reading Dataset\n",
        "  texts = read_dataset(file)\n",
        "\n",
        "  #Removing punctuations from datapoints\n",
        "  corpus = rempunct(texts)\n",
        "\n",
        "  #Filtering Proper Nouns\n",
        "  filtered = filter_nouns(corpus)\n",
        "  \n",
        "  #Tokenizing CSV\n",
        "  tokens = tokenize_csv(corpus)\n",
        "  \n",
        "  #Lemmatizing\n",
        "  if(lemma == 1):\n",
        "    tokens = lemmatize(tokens)\n",
        "  \n",
        "  #Creating 2 length phrases\n",
        "  tokens2 = create_nphrase(tokens, 2)\n",
        "  \n",
        "  #Creating 3 length phrases\n",
        "  tokens3 = create_nphrase(tokens, 3)\n",
        "\n",
        "  #Removing Stop words\n",
        "  filtered = None\n",
        "  filtered = remove_stopwords(tokens)\n",
        "\n",
        "  #Converting tokens to words\n",
        "  filtered = addspaces(filtered)\n",
        "\n",
        "  #Making Features\n",
        "  #Length 1\n",
        "  features_1 = make_features(filtered, thresh1)\n",
        "\n",
        "  #Length 2\n",
        "  features_2 = make_features(tokens2, thresh2)\n",
        "\n",
        "  #Length 3\n",
        "  features_3 = make_features(tokens3, thresh3)\n",
        "  \n",
        "  features = features_1 + features_2 + features_3\n",
        "\n",
        "  return features\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9_CGuo7Dpg5"
      },
      "source": [
        "job = features_csv('Job.csv', 100, 400, 150)\n",
        "print(\"job : \", len(job))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_eUCMYX9HNb",
        "outputId": "50db3d05-7120-46cf-c52e-72ff41a8b752"
      },
      "source": [
        "gossip1 = features_csv('friends_quotes 3.csv', 250, 50, 36)\n",
        "print(\"gossip : \", len(gossip1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' move ', 250)\n",
            "270\n",
            "(' for ya ', 50)\n",
            "1548\n",
            "(' to you about ', 36)\n",
            "520\n",
            "gossip :  2338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnn-0rEPtCz3",
        "outputId": "951848ce-9d23-41b4-84fb-675c2204c4fd"
      },
      "source": [
        "tech = features_csv('tech.csv', 50, 40, 10)\n",
        "print(\"tech : \", len(tech))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' film ', 50)\n",
            "239\n",
            "(' need to ', 40)\n",
            "148\n",
            "(' 2004 according to ', 10)\n",
            "183\n",
            "tech :  570\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22YRzlg_w1ra",
        "outputId": "4faa2c42-ba95-405b-9941-86469af81bc8"
      },
      "source": [
        "business = features_csv('business.csv', 20, 10, 10)\n",
        "print(\"business : \", len(business))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' developing ', 20)\n",
            "695\n",
            "(' in fact ', 10)\n",
            "935\n",
            "(' the two countries ', 10)\n",
            "161\n",
            "business :  1791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZZXNZwDvqTJ",
        "outputId": "c8ccdc0d-fe73-40d1-aed7-df6bc2cd39c3"
      },
      "source": [
        "medical = features_csv('/content/drive/MyDrive/SwiftChat/Dataframes/Cleaned/medicalcleaned.csv', 30, 30, 10)\n",
        "print(\"medical : \", len(medical))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' forward ', 30)\n",
            "868\n",
            "(' total cases ', 30)\n",
            "378\n",
            "(' active cases below ', 10)\n",
            "666\n",
            "medical :  1912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-phJfRxFo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5ca819-8293-47e5-dd66-270ef149b6a3"
      },
      "source": [
        "gossip1 = features_csv('friends_quotes 3.csv', 700, 50, 36)\n",
        "news2 = features_csv('SwiftChat Resources.csv', 5, 2, 1)\n",
        "f_features = final_features(gossip1, news2)\n",
        "print(\"gossip : \" , len(gossip1))\n",
        "print(\"news : \" , len(news2))\n",
        "print(\"Total : \" , len(f_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gossip :  2166\n",
            "news :  1989\n",
            "Total :  4120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REqBAk8kIdfd"
      },
      "source": [
        "friends = read_dataset('friends_quotes 3.csv')\n",
        "resources = read_dataset('SwiftChat Resources.csv')\n",
        "gossip1_df = make_processable(friends)\n",
        "news1_df = make_processable(resources)\n",
        "print(gossip1_df[0])\n",
        "print(len(gossip1_df))\n",
        "print(news1_df[0])\n",
        "print(len(news1_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiIAnGhNuE4-"
      },
      "source": [
        "gossip_dataframe = define_features(gossip1_df, f_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrmGxYhSKC_Y"
      },
      "source": [
        "news_dataframe = define_features(news1_df, f_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b61SlJUp7Ui7"
      },
      "source": [
        "features = []\n",
        "for token in medical:\n",
        "  temp = []\n",
        "  temp.append(token)\n",
        "  features.append(temp)\n",
        "\n",
        "import csv\n",
        "\n",
        "file = open('/content/drive/MyDrive/SwiftChat/Features/medicalfeatures.csv', 'w+', newline='')\n",
        "\n",
        "with file:\n",
        "  write = csv.writer(file)\n",
        "  write.writerows(features)"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}